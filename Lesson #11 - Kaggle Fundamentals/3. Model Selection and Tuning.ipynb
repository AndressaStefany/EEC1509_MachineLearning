{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Model Selection and Tuning.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"RrthG7Nha0bX","colab_type":"text"},"cell_type":"markdown","source":["# 1 - Introducuing Model Selection\n"]},{"metadata":{"id":"oV9Z_L5QbdA-","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","In the previous mission, we worked to optimize our predictions by creating and selecting the features used to train our model. The other half of the optimization puzzle is to optimize the model itself— or more specifically, the algorithm used to train our model.\n","\n","So far, we've been using the **logistic regression** algorithm to train our models, however there are hundreds of different machine learning algorithms from which we can choose. Each algorithm has different strengths and weaknesses, and so we need to select the algorithm that works best with our specific data— in this case our Kaggle competition.\n","\n","The process of selecting the algorithm which gives the best predictions for your data is called **model selection**.\n","\n","In this mission, we're going work with two new algorithms: **k-nearest neighbors** and **random forests**.\n","\n","Before we begin, we'll need to import in the data. To save time, we have saved the features we created in the previous mission as CSV files, **train_modified.csv** and **holdout_modified.csv**."]},{"metadata":{"id":"iabWjxMOa0bX","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","train = pd.read_csv('train_modified.csv')\n","holdout = pd.read_csv('holdout_modified.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iBMeoQMwa0bc","colab_type":"code","colab":{}},"cell_type":"code","source":["train.columns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NomvFMW1a0bh","colab_type":"code","colab":{}},"cell_type":"code","source":["holdout.columns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iGKHDtw3a0bl","colab_type":"text"},"cell_type":"markdown","source":["# 2 - Trainning a Baseline Model\n"]},{"metadata":{"id":"FgQkwSMIsdtx","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","We're going to train our models using all the columns in the **train** dataframe. This will cause a small amount of overfitting due to collinearity (as we discussed in the previous mission), but having more features will allow us to more thoroughly compare algorithms.\n","\n","So we have something to compare to, we're going to train a logistic regression model like in the previous two missions. We'll use cross validation to get a baseline score."]},{"metadata":{"id":"JpF-Gzwha0bm","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","\n","all_X = train.drop(['Survived','PassengerId'],axis=1)\n","all_y = train['Survived']\n","lr = LogisticRegression()\n","scores = cross_val_score(lr, all_X, all_y, cv=10)\n","accuracy_lr = scores.mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FvDBnYMta0bo","colab_type":"code","colab":{}},"cell_type":"code","source":["accuracy_lr"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D_o-LmOfa0br","colab_type":"text"},"cell_type":"markdown","source":["# 3 - Training a Model using K-Nearest Neighbors\n"]},{"metadata":{"id":"Rx7CUspote8e","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The logistic regression baseline model from the previous screen scored 82.38%.\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.26%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","\n","The logistic regression algorithm works by calculating linear relationships between the features and the target variable and using those to make predictions. Let's look at an algorithm that makes predictions using a different method.\n","\n","The **k-nearest neighbors** algorithm finds the observations in our training set most similar to the observation in our test set, and uses the average outcome of those 'neighbor' observations to make a prediction. The **'k'** is the number of neighbor observations used to make the prediction.\n","\n","The plots below shows three simple k-nearest neighbors models where there are two features shown on each axis, and two outcomes, red and green.\n","\n","<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1kDp8LCdZpDJF-3HsEQIAm7BQEJtU1I0m\">\n","\n","\n","- In the first plot, the value of k is 1, so the closest 1 neighbour to our gray dot is used, green, making the prediction **green**.\n","- In the second plot, the value of k is 3, so the closest 3 neighbours to our gray dot are used, green, making the prediction **red** (2 red vs 1 green).\n","- In the third plot, the value of k is 5, so the closest 5 neighbours to our gray dot are used, green, making the prediction **red** (3 red vs 2 green).\n","\n","\n","Just like it does for logistic regression, scikit-learn has a class that makes it easy to use k-nearest neighbors to make predictions, [neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n","\n","Scikit-learn's use of object-oriented design makes it easy to substitute one model for another. The syntax to instantiate a **KNeighborsClassifier** is very similar to the syntax we use for logistic regression.\n","\n",">```python\n","from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors=1)\n","```\n","\n","The optional **n_neighbors** argument sets the value of k when predictions are made. The default value of **n_neighbors** is 5, but we're going to start by building a simple model that uses the closest neighbor to make our predictions."]},{"metadata":{"id":"sTbMp3vZa0bs","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors=1)\n","\n","scores = cross_val_score(knn, all_X, all_y, cv=10)\n","accuracy_knn = scores.mean()\n","accuracy_knn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vAM8a293a0bv","colab_type":"text"},"cell_type":"markdown","source":["# 4 - Exploring Different K Values\n"]},{"metadata":{"id":"M7Gcuti8txkq","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The k-nearest neighbors model we trained in the previous screen had an accuracy score of 78.57%, worse than our baseline score of 82.38%.\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.26%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","| K-nearest neighbors, k == 1  | 78.57%                  |              |\n","\n","\n","Besides pure model selection, we can vary the settings of each model— for instance the value of k in our k-nearest neighbors model. This is called **hyperparameter optimization**.\n","\n","We can use a loop and Python's inbuilt [range() class](https://docs.python.org/3/library/stdtypes.html#range) to iterate through different values for **k** and calculate the accuracy score for each different value. We will only want to test odd values for **k** to avoid ties, where both 'survived' and 'died' outcomes would have the same number of neighbors.\n","\n","This is the syntax we would use to get odd values between 1-7 from **range()**:\n","\n",">```python\n",">>>  for k in range(1,8,2):\n","...      print(k)\n","     1\n","     3\n","     5\n","     7\n","```\n","\n","Note that we use the arguments (1,8,2) to get values between 1 and 7, since the created range() object contains numbers up to but not including the 8.\n","\n","Let's use this technique to calculate the accuracy of our model for values of k from 1-49, storing the results in a dictionary.\n","\n","To make the results easier to understand, we'll finish by plotting the scores. We have provided a helper function, plot_dict() which you can use to easily plot the dictionary.\n","\n","Note that we expect this step to take a while to run, as we are training 250 models in total (10 cross validation models for each of 25 values of k)."]},{"metadata":{"id":"OjzMXk_Ka0bw","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","sns.set()\n","\n","def plot_dict(dictionary):\n","    pd.Series(dictionary).plot.bar(figsize=(9,6),\n","                                   ylim=(0.78,0.83),rot=0)\n","    plt.show()\n","\n","knn_scores = dict()\n","for k in range(1,50,2):\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","\n","    scores = cross_val_score(knn, all_X, all_y, cv=10)\n","    accuracy_knn = scores.mean()\n","    knn_scores[k] = accuracy_knn\n","\n","plot_dict(knn_scores)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xVrsJCsoa0b0","colab_type":"code","colab":{}},"cell_type":"code","source":["knn = KNeighborsClassifier(n_neighbors=19)\n","\n","scores = cross_val_score(knn, all_X, all_y, cv=10)\n","accuracy_knn = scores.mean()\n","accuracy_knn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ax3ljM0la0b3","colab_type":"text"},"cell_type":"markdown","source":["# 5 - Automating Hyperparameter Optimization with Grid Search\n"]},{"metadata":{"id":"8w16rx3Wt-VU","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","Looking at our plot from the previous screen we can see that a k value of 19 gave us our best score, and checking the knn_scores dictionary we can see that the score was 82.38%, identical to our baseline (if we didn't round the numbers you would see that it's actually 0.01% less accurate).\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.36%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","| K-nearest neighbors, k == 1  | 78.57%                  |              |\n","| K-nearest neighbors, k == 19 | 82.38%                  |              |\n","\n","\n","\n","The technique we just used is called **grid search** - we train a number of models across a 'grid' of values and then searched for the model that gave us the highest accuracy.\n","\n","Scikit-learn has a class to perform grid search, [model_selection.GridSearchCV()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). The 'CV' in the name indicates that we're performing both grid search and cross validation at the same time.\n","\n","By creating a dictionary of parameters and possible values and passing it to the **GridSearchCV** object you can automate the process. Here's what the code from the previous screen would look like, when implemented using the **GridSearchCV** class.\n","\n",">```python\n","from sklearn.model_selection import GridSearchCV\n","knn = KNeighborsClassifier()\n","hyperparameters = {\n","    \"n_neighbors\": range(1,50,2)\n","}\n","grid = GridSearchCV(knn, param_grid=hyperparameters, cv=10)\n","grid.fit(all_X, all_y)\n","print(grid.best_params_)\n","print(grid.best_score_)\n","```\n","\n","Running this code will produce the following output:\n","\n",">```python\n","{'n_neighbors': 19}\n","0.8238914\n","```\n","\n","Our final step was to print the **GridSearchCV.best\\_params\\_** and **GridSearchCV.best\\_score\\_** attributes to retrieve the parameters of the best-performing model, and the score it achieved.\n","\n","We can also use **GridSearchCV** to try combinations of different hyperparameters. Say we wanted to test values of **\"ball_tree\"**, **\"kd_tree\"**, and **\"brute\"** for the **algorithm** parameter and values of **1**, **3**, and **5** for the **n_neighbors** algorithm parameter. **GridSearchCV** would train and test 9 models (3 for the first hyperparameter times 3 for the second hyperparameter), shown in the diagram below.\n","\n","<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=16EfPLZ7AWaW4XPwKtn6D9OWBY_alh-YF\">\n","\n","Let's use **GridSearchCV** to turbo-charge our search for the best performing parameters for our model, by testing 40 combinations of three different hyperparameters.\n","\n","We have chosen the specific hyperparameters by consulting the documentation for the **KNeighborsClassifier** class.\n","\n","\n"]},{"metadata":{"id":"Vh8yYGrZa0b5","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","hyperparameters = {\n","    \"n_neighbors\": range(1,20,2),\n","    \"weights\": [\"distance\", \"uniform\"],\n","    \"algorithm\": ['brute'],\n","    \"p\": [1,2]\n","}\n","knn = KNeighborsClassifier()\n","grid = GridSearchCV(knn,param_grid=hyperparameters,cv=10)\n","\n","grid.fit(all_X, all_y)\n","\n","best_params = grid.best_params_\n","best_score = grid.best_score_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nh6In4Jva0b6","colab_type":"code","colab":{}},"cell_type":"code","source":["print(best_params)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qLgY2hC_a0b-","colab_type":"code","colab":{}},"cell_type":"code","source":["print(best_score)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"faTIOw0ka0cA","colab_type":"text"},"cell_type":"markdown","source":["# 6 - Submitting K-Nearest Neighbors Predictions to Kaggle\n"," "]},{"metadata":{"id":"brZuOgeOuO0-","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The cross-validation score for the best performing model was 82.82%, better than our baseline model.\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.36%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","| K-nearest neighbors, k == 1  | 78.57%                  |              |\n","| K-nearest neighbors, k == 19 | 82.38%                  |              |\n","| K-nearest neighbors, best model from grid search | 82.82%                  |              |\n","\n","\n","We can use the **GridSearchCV.best\\_estimator\\_** attribute to retrieve a trained model with the best-performing hyperparameters. This code:\n","\n",">```python\n","best_knn = grid.best_estimator_\n","```\n","\n","is equivalent to this code where we manually specify the hyperparameters and train the model:\n","\n",">```python\n","best_knn = KNeighborsClassifier(p=1,algorithm='brute',n_neighbors=5,\n","                     weights='uniform')\n","best_knn.fit(all_X,all_y)\n","```\n","\n","Lets use that model to make predictions on the holdout set and submit those predictions to Kaggle to see if we have improved overall."]},{"metadata":{"id":"sge8vh2ra0cB","colab_type":"code","colab":{}},"cell_type":"code","source":["holdout_no_id = holdout.drop(['PassengerId'],axis=1)\n","\n","best_knn = grid.best_estimator_\n","holdout_predictions = best_knn.predict(holdout_no_id)\n","\n","holdout_ids = holdout[\"PassengerId\"]\n","submission_df = {\"PassengerId\": holdout_ids,\n","                 \"Survived\": holdout_predictions}\n","submission = pd.DataFrame(submission_df)\n","\n","submission.to_csv(\"submission_4.csv\",index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4gcVIpSna0cF","colab_type":"text"},"cell_type":"markdown","source":["# 7 - Introducing Random Forests\n"]},{"metadata":{"id":"WTHFzu0juVLg","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","When you submit this toKaggle, you'll see it scores 75.59%, less than our best submission of 78.48%. While our model could be overfitting due to including all columns, it also seems like k-nearest neighbors may not be the best algorithm choice.\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.36%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","| K-nearest neighbors, k == 1  | 78.57%                  |              |\n","| K-nearest neighbors, k == 19 | 82.38%                  |              |\n","| K-nearest neighbors, best model from grid search | 82.82%                  |    75.59%          |\n","\n","\n","Let's try another algorithm called **random forests**. Random forests is a specific type of **decision tree** algorithm. You have likely seen decision trees before as part of flow charts or infographics. Say we wanted to build a decision tree to help us [categorize an object as either being 'hotdog' or 'not hotdog'](https://www.youtube.com/watch?v=ACmydtFDTGs), we could construct a decision tree like the below:\n","\n","\n","<img width=\"600\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=110mct28LEeOqgX882oEab-Uxw2lEp9cD\">\n","\n","\n","Decision tree algorithms attempt to build the most efficient decision tree based on the training data, and then use that tree to make future predictions.\n","\n","\n","Scikit-learn contains a class for classification using the random forest algorithm, [ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Here's how to fit a model and make predictions using the **RandomForestClassifier** class:\n","\n",">```python\n","from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier(random_state=1)\n","clf.fit(train_X,train_y)\n","predictions = clf.predict(test_X)\n","```\n","\n","Because the algorithm includes randomization, we have to set the **random_state** parameter to make sure our results are reproducible.\n","\n","Let's use a **RandomForestClassifier** object with **cross_val_score()** as we did earlier to see how the algorithm performs with the default hyperparameters."]},{"metadata":{"id":"zqbYhf7ma0cH","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","clf = RandomForestClassifier(random_state=1)\n","scores = cross_val_score(clf, all_X, all_y, cv=10)\n","accuracy_rf = scores.mean()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DsjTuHgma0cK","colab_type":"code","colab":{}},"cell_type":"code","source":["accuracy_rf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JiYICe5Sa0cP","colab_type":"text"},"cell_type":"markdown","source":["#8 - Tuning our Random Forests Model with GridSearch\n"]},{"metadata":{"id":"mwszAmPhubQQ","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","Using the default settings, our random forests model obtained a cross validation score of 80.70%.\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.36%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","| K-nearest neighbors, k == 1  | 78.57%                  |              |\n","| K-nearest neighbors, k == 19 | 82.38%                  |              |\n","| K-nearest neighbors, best model from grid search | 82.82%                  |    75.59%          |\n","| Random forests, default hyperparameters | 80.70%                  |              |\n","\n","\n","Just like we did with the k-nearest neighbors model, we can use GridSearchCV to test a variety of hyperparameters to find the best performing model.\n","\n","The best way to see a list of available hyperparameters is by checking the documentation for the classifier— in this case, the [documentation for RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Let's use grid search to test out combinations of the following hyperparameters:\n","\n","- criterion: \"entropy\" or \"gini\"\n","- max_depth: 2, 5, 8 or 10\n","- max_features: \"log2\" or \"sqrt\"\n","- min_samples_leaf: 1, 2, 3, 4 or 5\n","- min_samples_split: 2, 3, 4 or 5\n","- n_estimators: 6, 9"]},{"metadata":{"id":"ENNeARyGa0cQ","colab_type":"code","colab":{}},"cell_type":"code","source":["hyperparameters = {\"criterion\": [\"entropy\", \"gini\"],\n","                   \"max_depth\": [2, 5, 8, 10],\n","                   \"max_features\": [\"log2\", \"sqrt\"],\n","                   \"min_samples_leaf\": [1, 2, 3, 4, 5],\n","                   \"min_samples_split\": [2, 3, 4, 5],\n","                   \"n_estimators\": [6, 9]\n","}\n","\n","clf = RandomForestClassifier(random_state=1)\n","grid = GridSearchCV(clf,param_grid=hyperparameters,cv=10)\n","\n","grid.fit(all_X, all_y)\n","\n","best_params = grid.best_params_\n","best_score = grid.best_score_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T5TD1xGGa0cV","colab_type":"code","colab":{}},"cell_type":"code","source":["best_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pzSAXthga0ca","colab_type":"code","colab":{}},"cell_type":"code","source":["best_params"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PbRWhUula0cf","colab_type":"text"},"cell_type":"markdown","source":["# 9 - Submitting Random Forest Predictions to Kaggle\n"]},{"metadata":{"id":"4nEEU_pnu9K_","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The cross-validation score for the best performing model was 84.28%, making it the best cross-validation score we've obtained in this mission.\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.36%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","| K-nearest neighbors, k == 1  | 78.57%                  |              |\n","| K-nearest neighbors, k == 19 | 82.38%                  |              |\n","| K-nearest neighbors, best model from grid search | 82.82%                  |    75.59%          |\n","| Random forests, default hyperparameters | 80.70%                  |              |\n","| Random forests, best model from grid search | 84.28%                  |              |\n","\n","Let's train it on the holdout data and create a submission file to see how it performs on the Kaggle leaderboard!"]},{"metadata":{"id":"AgNiayJGa0ch","colab_type":"code","colab":{}},"cell_type":"code","source":["# The `GridSearchCV` object is stored in memory from\n","# the previous screen with the variable name `grid`\n","best_rf = grid.best_estimator_\n","holdout_predictions = best_rf.predict(holdout_no_id)\n","\n","holdout_ids = holdout[\"PassengerId\"]\n","submission_df = {\"PassengerId\": holdout_ids,\n","                 \"Survived\": holdout_predictions}\n","submission = pd.DataFrame(submission_df)\n","\n","submission.to_csv(\"submission_5.csv\",index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jskAkjCVa0cj","colab_type":"text"},"cell_type":"markdown","source":["#10 - Next steps\n"]},{"metadata":{"id":"BfVFMJx5vAu4","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","If you submit this to Kaggle, it achieves a score of 77.55%, considerably better than our k-nearest neighbors score of 75.59% and very close  to our best score from the previous mission of 78.48%\n","\n","| Model                        | Cross-validation score | Kaggle score |\n","|------------------------------|------------------------|--------------|\n","| Previous best Kaggle score   | 82.36%                  | 78.48%        |\n","| Logistic regression baseline | 82.38%                  |              |\n","| K-nearest neighbors, k == 1  | 78.57%                  |              |\n","| K-nearest neighbors, k == 19 | 82.38%                  |              |\n","| K-nearest neighbors, best model from grid search | 82.82%                  |    75.59%          |\n","| Random forests, default hyperparameters | 80.70%                  |              |\n","| Random forests, best model from grid search | 84.28%                  |       77.55%       |\n","\n","By combining our strategies for feature selection, feature engineering, model selection and model tuning, we'll be able to continue to improve our score.\n","\n","The next and final mission in this course is a guided project, where we'll teach you how to combine everything you've learned into a real-life Kaggle workflow, and continue to improve your score."]}]}