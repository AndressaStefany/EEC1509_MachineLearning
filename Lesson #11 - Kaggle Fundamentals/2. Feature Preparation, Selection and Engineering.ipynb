{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Feature Preparation, Selection and Engineering.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"7KEjpGkvlJpa","colab_type":"text"},"cell_type":"markdown","source":["# 1 - Introduction\n"]},{"metadata":{"id":"v3UU-QXPlTFF","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","In the last mission, we made our first submission to Kaggle, getting an accuracy score of 75.6%. While this is a good start, there is definitely room for improvement. There are two main areas we can focus on to boost the accuracy of our predictions:\n","\n","- Improving the features we train our model on\n","- Improving the model itself\n","\n","In this mission, we're going to focus working with the features used in our model.\n","\n","We'll start by looking at **feature selection**. Feature selection is important because it helps to exclude features which are not good predictors, or features that are closely related to each other. Both of these will cause our model to be less accurate, particularly on previously unseen data.\n","\n","The diagram below illustrates this. The red dots represent the data we are trying to predict, and each of the blue lines represents a different model.\n","\n","<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1kbrNc65A77r6QSOZpyX79WUdPhCVbVlb\">\n","\n","\n","The model on the left is **overfitting**, which means the model represents the training data too closely, and is unlikely to predict well on unseen data, like the holdout data for our Kaggle competition.\n","\n","The model on the right is **well-fit**. It captures the underlying pattern in the data without the detailed noise found just in the training set. A well fit model is likely to make accurate predictions on previously unseen data. The key to creating a well-fit model is to select the right balance of features, and to create new features to train your model.\n","\n","In the previous mission, we trained our model using data about the age, sex and class of the passengers on the Titanic. Let's start by using the functions we created in that mission to add the columns we had at the end of the first mission.\n","\n","Remember that any modifications we make to our training data (**train.csv**) we also have to make to our holdout data (**test.csv**).\n","\n"]},{"metadata":{"id":"yPxNFxTmlJpc","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","\n","train = pd.read_csv('train.csv')\n","holdout = pd.read_csv('test.csv')\n","\n","def process_age(df):\n","    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n","    cut_points = [-1,0,5,12,18,35,60,100]\n","    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n","    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n","    return df\n","\n","def create_dummies(df,column_name):\n","    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n","    df = pd.concat([df,dummies],axis=1)\n","    return df\n","  \n","train = process_age(train)\n","holdout = process_age(holdout)\n","\n","for column in [\"Age_categories\",\"Pclass\",\"Sex\"]:\n","    train = create_dummies(train,column)\n","    holdout = create_dummies(holdout,column)\n","\n","print(train.columns)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IBG-dlFjlJpi","colab_type":"text"},"cell_type":"markdown","source":["# 2 -  Preparing More Features\n"]},{"metadata":{"id":"Pzb7Okjlm1Nm","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","Our model in the previous mission was based on three columns from the original data: **Age**, **Sex**, and **Pclass**. As you saw when you printed the column names in the previous screen, there are a number of other columns that we haven't yet used. To make it easier to reference, the output from the previous screen is copied below:\n","\n","```python\n","Index(['PassengerId', 'Survived', 'Pclass', 'Name',\n","       'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n","       'Fare', 'Cabin', 'Embarked', 'Age_categories',\n","       'Age_categories_Missing',\n","       'Age_categories_Infant',\n","       'Age_categories_Child',\n","       'Age_categories_Teenager',\n","       'Age_categories_Young Adult',\n","       'Age_categories_Adult',\n","       'Age_categories_Senior', 'Pclass_1',\n","       'Pclass_2', 'Pclass_3','Sex_female',\n","       'Sex_male'], dtype='object')\n","```\n","\n","We can ignore **PassengerId**, since this is just a column Kaggle have added to identify each passenger and calculate scores. We can also ignore **Survived**, as this is what we're predicting, as well as the three columns we've already used.\n","\n","Here is a list of the remaining columns (with a brief description), followed by 5 randomly selected passengers from and their data from those columns, so we can refamiliarize ourselves with the data.\n","\n","- **SibSp** - The number of siblings or spouses the passenger had aboard the Titanic\n","- **Parch** - The number of parents or children the passenger had aboard the Titanic\n","- **Ticket** - The passenger's ticket number\n","- **Fare** - The fair the passenger paid\n","- **Cabin** - The passengers cabin number\n","- **Embarked** - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)"]},{"metadata":{"id":"0HvtRTxplJpn","colab_type":"text"},"cell_type":"markdown","source":["At first glance, both the **Name** and **Ticket** columns look to be unique to each passenger. We will come back to these columns later, but for now we'll focus on the other columns.\n","\n","We can use the **Dataframe.describe()** method to give us some more information on the values within each remaining column."]},{"metadata":{"scrolled":true,"id":"7w1QP5S8lJpn","colab_type":"code","colab":{}},"cell_type":"code","source":["columns = ['SibSp','Parch','Fare','Cabin','Embarked']\n","train[columns].describe(include='all')"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"j4vPSbgNlJpr","colab_type":"code","colab":{}},"cell_type":"code","source":["train[columns].isnull().sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F_M4oSmLlJpt","colab_type":"code","colab":{}},"cell_type":"code","source":["holdout[columns].isnull().sum()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nB8hO8VulJpx","colab_type":"text"},"cell_type":"markdown","source":["Of these, **SibSp**, **Parch** and **Fare** look to be standard numeric columns with no missing values. **Cabin** has values for only 204 of the 891 rows, and even then most of the values are unique, so for now we will leave this column also. **Embarked** looks to be a standard categorical column with 3 unique values, much like **PClass** was, except that there are two missing values. We can easily fill these two missing values with the most common value, **\"S\"** which occurs 644 times.\n","\n","Looking at our numeric columns, we can see a big difference between the range of each. **SibSp** has values between 0-8, **Parch** between 0-6, and **Fare** is on a dramatically different scale, with values ranging from 0-512. In order to make sure these values are equally weighted within our model, we'll need to **rescale** the data.\n","\n","Rescaling simply stretches or shrinks the data as needed to be on the same scale, in our case between 0 and 1.\n","\n","<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1gUe_kSss54VwGzmveizevZsFYB5lk3u1\">\n","\n","\n","After rescaling, the values in each feature has been compressed or stretched so that they are all on the same scale - they have the same minimum and maximum, and the relationship between each point is still the same relative other points in that feature. You can now easily see that the data represented in each column is identical.\n","\n","Within scikit-learn, the [preprocessing.minmax_scale()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) function allows us to quickly and easily rescale our data:\n","\n","```python\n","from sklearn.preprocessing import minmax_scale\n","columns = [\"column one\", \"column two\"]\n","data[columns] = min_max_scale(data[columns])\n","```\n","\n","Let's process the **Embarked**, **SibSp**, **Parch** and **Fare** columns in both our **train** and **holdout** dataframes.\n","\n"]},{"metadata":{"id":"w0sBfgyglJpy","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import minmax_scale\n","# The holdout set has a missing value in the Fare column which\n","# we'll fill with the mean.\n","holdout[\"Fare\"] = holdout[\"Fare\"].fillna(train[\"Fare\"].mean())\n","columns = [\"SibSp\",\"Parch\",\"Fare\"]\n","\n","train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n","holdout[\"Embarked\"] = holdout[\"Embarked\"].fillna(\"S\")\n","\n","train = create_dummies(train,\"Embarked\")\n","holdout = create_dummies(holdout,\"Embarked\")\n","\n","for col in columns:\n","    train[col + \"_scaled\"] = minmax_scale(train[col])\n","    holdout[col + \"_scaled\"] = minmax_scale(holdout[col])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dq8TAF8jlJp4","colab_type":"code","colab":{}},"cell_type":"code","source":["train.columns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F8I6I2EAlJp7","colab_type":"text"},"cell_type":"markdown","source":["# 3 - Determining the Most Relevant Features\n"]},{"metadata":{"id":"zAfsNWCau8WX","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","In order to select the best-performing features, we need a way to measure which of our features are relevant to our outcome - in this case, the survival of each passenger. One effective way is by training a logistic regression model using all of our features, and then looking at the coefficients of each feature.\n","\n","The scikit-learn [LogisticRegression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) has an attribute in which coefficients are stored after the model is fit, **LogisticRegression.coef_**. We first need to train our model, after which we can access this attribute.\n","\n",">```python\n","lr = LogisticRegression()\n","lr.fit(train_X,train_y)\n","coefficients = lr.coef_\n","```\n","\n","The **coef()** method returns a NumPy array of coefficients, in the same order as the features that were used to fit the model. To make these easier to interpret, we can convert the coefficients to a pandas series, adding the column names as the index:\n","\n",">```python\n","feature_importance = pd.Series(coefficients[0],\n","                               index=train_X.columns)\n","```\n","\n","We'll now fit a model and plot the coefficients for each feature."]},{"metadata":{"scrolled":true,"id":"VxZwasuPlJp8","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.linear_model import LogisticRegression\n","\n","sns.set()\n","\n","columns = ['Age_categories_Missing', 'Age_categories_Infant',\n","       'Age_categories_Child', 'Age_categories_Teenager',\n","       'Age_categories_Young Adult', 'Age_categories_Adult',\n","       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n","       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n","       'SibSp_scaled', 'Parch_scaled', 'Fare_scaled']\n","\n","lr = LogisticRegression()\n","lr.fit(train[columns],train['Survived'])\n","\n","coefficients = lr.coef_\n","\n","feature_importance = pd.Series(coefficients[0],\n","                               index=columns)\n","feature_importance.plot.barh()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hTZ0vSu4lJp_","colab_type":"text"},"cell_type":"markdown","source":["# 4 - Training a model using relevant features.\n"]},{"metadata":{"id":"KxJUfuwSwja0","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The plot we generated in the last section showed a range of both positive and negative values. Whether the value is positive or negative isn't as important in this case, relative to the magnitude of the value. If you think about it, this makes sense. A feature that indicates strongly whether a passenger died is just as useful as a feature that indicates strongly that a passenger survived, given they are mutually exclusive outcomes.\n","\n","To make things easier to interpret, we'll alter the plot to show all positive values, and have sorted the bars in order of size:"]},{"metadata":{"id":"RdVHR2jQlJqA","colab_type":"code","colab":{}},"cell_type":"code","source":["ordered_feature_importance = feature_importance.abs().sort_values()\n","ordered_feature_importance.plot.barh()\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OMu8Wl8llJqD","colab_type":"text"},"cell_type":"markdown","source":["We'll train a new model with the top 8 scores and check our accuracy using cross validation."]},{"metadata":{"id":"QtFstIoYlJqE","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","\n","columns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n","       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']\n","all_X = train[columns]\n","all_y = train['Survived']\n","\n","lr = LogisticRegression()\n","scores = cross_val_score(lr, all_X, all_y, cv=10)\n","accuracy = scores.mean()\n","print(accuracy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xStcCjh1lJqI","colab_type":"text"},"cell_type":"markdown","source":["# 5 - Submitting our Improved Model to Kaggle\n"]},{"metadata":{"id":"RwLviFmCxJ8z","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The cross validation score of 81.48% is marginally higher than the cross validation score for the model we created in the previous mission, which had a score of 80.2%.\n","\n","Hopefully, this improvement will translate to previously unseen data. Let's train a model using the columns from the previous step, make some predictions on the holdout data and submit it to Kaggle for scoring."]},{"metadata":{"id":"QLH7mjwSlJqK","colab_type":"code","colab":{}},"cell_type":"code","source":["columns = ['Age_categories_Infant', 'SibSp_scaled', 'Sex_female', 'Sex_male',\n","       'Pclass_1', 'Pclass_3', 'Age_categories_Senior', 'Parch_scaled']\n","\n","all_X = train[columns]\n","all_y = train['Survived']\n","\n","lr = LogisticRegression()\n","lr.fit(all_X,all_y)\n","holdout_predictions = lr.predict(holdout[columns])\n","\n","holdout_ids = holdout[\"PassengerId\"]\n","submission_df = {\"PassengerId\": holdout_ids,\n","                 \"Survived\": holdout_predictions}\n","submission = pd.DataFrame(submission_df)\n","\n","submission.to_csv(\"submission_2.csv\",index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4nnW4R70lJqM","colab_type":"text"},"cell_type":"markdown","source":["# 6 - Engineering a New Feature Using Binning\n"]},{"metadata":{"id":"pkClkCD9xVS6","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","When you submit it to Kaggle, you'll see that the store is 77.33%, which at the time of writing equates to jumping about 2,017 places up the leaderboard (this will vary as the leaderboard is always changing). It's only a small improvement, but we're moving in the right direction.\n","\n","A lot of the gains in accuracy in machine learning come from **Feature Engineering**. Feature engineering is the practice of creating new features from your existing data.\n","\n","One common way to engineer a feature is using a technique called **binning**. Binning is when you take a continuous feature, like the fare a passenger paid for their ticket, and separate it out into several ranges (or 'bins'), turning it into a categorical variable.\n","\n","**This can be useful when there are patterns in the data that are non-linear and you're using a linear model (like logistic regression)**. We actually used binning in the previous mission when we dealt with the **Age** column, although we didn't use the term.\n","\n","Let's look at histograms of the **Fare** column for passengers who died and survived, and see if there are patterns that we can use when creating our bins."]},{"metadata":{"id":"VjNlD2oWlJqO","colab_type":"code","colab":{}},"cell_type":"code","source":["survived = train[train[\"Survived\"] == 1]\n","died = train[train[\"Survived\"] == 0]\n","\n","survived[\"Fare\"].plot.hist(alpha=0.5,color='red',bins=50)\n","died[\"Fare\"].plot.hist(alpha=0.5,color='blue',bins=50)\n","plt.legend(['Survived','Died'])\n","plt.xlim(0,140)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qDnZwOs2lJqS","colab_type":"text"},"cell_type":"markdown","source":["Looking at the values, it looks like we can separate the feature into four bins to capture some patterns from the data:\n","\n","- 0-12\n","- 12-50\n","- 50-100\n","- 100+\n","\n","Like in the previous mission, we can use the **pandas.cut()** function to create our bins."]},{"metadata":{"id":"JtoCRDxolJqT","colab_type":"code","colab":{}},"cell_type":"code","source":["def process_fare(df,cut_points,label_names):\n","    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n","    return df\n","\n","cut_points = [0,12,50,100,1000]\n","label_names = [\"0-12\",\"12-50\",\"50-100\",\"100+\"]\n","\n","train = process_fare(train,cut_points,label_names)\n","holdout = process_fare(holdout,cut_points,label_names)\n","\n","train = create_dummies(train,\"Fare_categories\")\n","holdout = create_dummies(holdout,\"Fare_categories\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T5ElvlvWlJqY","colab_type":"code","colab":{}},"cell_type":"code","source":["train.columns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sljaljlHlJqc","colab_type":"text"},"cell_type":"markdown","source":["# 7 - Engineering Features From Text Columns\n"]},{"metadata":{"id":"8n9q3FLnyRHf","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","Another way to engineer features is by extracting data from text columns. Earlier, we decided that the **Name** and **Cabin** columns weren't useful by themselves, but what if there is some data there we could extract? Let's take a look at a random sample of rows from those two columns:\n","\n","|  |        Name                                   |   Cabin         |\n","|------|---------------------------------------------------|---------|\n","| 772  | Mack, Mrs. (Mary)                                 | E77     |\n","| 148  | Navratil, Mr. Michel (\"Louis M Hoffman\")          | F2      |\n","| 707  | Calderhead, Mr. Edward Pennington                 | E24     |\n","| 879  | Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)     | C50     |\n","| 21   | Beesley, Mr. Lawrence                             | D56     |\n","| 456  | Millet, Mr. Francis Davis                         | E38     |\n","| 97   | Greenfield, Mr. William Bertram                   | D10 D12 |\n","| 263  | Harrison, Mr. William                             | B94     |\n","| 393  | Newell, Miss. Marjorie                            | D36     |\n","| 759  | Rothes, the Countess. of (Lucy Noel Martha Dye... | B77     |\n","\n","While in isolation the cabin number of each passenger will be reasonably unique to each, we can see that the format of the cabin numbers is one letter followed by two numbers. It seems like the letter is representative of the type of cabin, which could be useful data for us. We can use the pandas [Series.str accessor](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.html) and then subset the first character using brackets:\n","\n","\n"]},{"metadata":{"id":"X68XAK5tlJqd","colab_type":"code","colab":{}},"cell_type":"code","source":["train.head()[\"Cabin\"]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JS2cfj5ZlJqh","colab_type":"code","colab":{}},"cell_type":"code","source":["train.head()[\"Cabin\"].str[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3YOUEcQXlJqk","colab_type":"text"},"cell_type":"markdown","source":["Looking at the **Name** column, There is a title like 'Mr' or 'Mrs' within each, as well as some less common titles, like the 'Countess' from the final row of our table above. By spending some time researching the different titles, we can categorize these into six types:\n","\n","- Mr\n","- Mrs\n","- Master\n","- Miss\n","- Officer\n","- Royalty\n","\n","We can use the [Series.str.extract](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html) method and a [regular expression](https://en.wikipedia.org/wiki/Regular_expression) to extract the title from each name and then use the [Series.map()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) method and a predefined dictionary to simplify the titles.\n","\n",">```python\n","titles = {\n","    \"Mme\":         \"Mrs\",\n","    \"Ms\":          \"Mrs\",\n","    \"Mrs\" :        \"Mrs\",\n","    \"Countess\":    \"Royalty\",\n","    \"Lady\" :       \"Royalty\"\n","}\n","extracted_titles = train[\"Name\"].str.extract('([A-Za-z]+)\\.', expand=False)\n","train[\"Title\"] = extracted_titles.map(titles)\n","```"]},{"metadata":{"id":"ezppquEOlJqn","colab_type":"code","colab":{}},"cell_type":"code","source":["titles = {\n","    \"Mr\" :         \"Mr\",\n","    \"Mme\":         \"Mrs\",\n","    \"Ms\":          \"Mrs\",\n","    \"Mrs\" :        \"Mrs\",\n","    \"Master\" :     \"Master\",\n","    \"Mlle\":        \"Miss\",\n","    \"Miss\" :       \"Miss\",\n","    \"Capt\":        \"Officer\",\n","    \"Col\":         \"Officer\",\n","    \"Major\":       \"Officer\",\n","    \"Dr\":          \"Officer\",\n","    \"Rev\":         \"Officer\",\n","    \"Jonkheer\":    \"Royalty\",\n","    \"Don\":         \"Royalty\",\n","    \"Sir\" :        \"Royalty\",\n","    \"Countess\":    \"Royalty\",\n","    \"Dona\":        \"Royalty\",\n","    \"Lady\" :       \"Royalty\"\n","}\n","\n","extracted_titles = train[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)\n","train[\"Title\"] = extracted_titles.map(titles)\n","\n","extracted_titles = holdout[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)\n","holdout[\"Title\"] = extracted_titles.map(titles)\n","\n","train[\"Cabin_type\"] = train[\"Cabin\"].str[0]\n","train[\"Cabin_type\"] = train[\"Cabin_type\"].fillna(\"Unknown\")\n","\n","holdout[\"Cabin_type\"] = holdout[\"Cabin\"].str[0]\n","holdout[\"Cabin_type\"] = holdout[\"Cabin_type\"].fillna(\"Unknown\")\n","\n","for column in [\"Title\",\"Cabin_type\"]:\n","    train = create_dummies(train,column)\n","    holdout = create_dummies(holdout,column)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"g0Rq1WRolJqp","colab_type":"code","colab":{}},"cell_type":"code","source":["train.columns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"783qBTyTlJqu","colab_type":"text"},"cell_type":"markdown","source":["# 8 - Finding Correlated Features\n"]},{"metadata":{"id":"S7eSDA3bzUgC","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","We now have a lot of possible feature columns we can use to train our model. One thing to be aware of as you start to add more features is a concept called **collinearity**. Collinearity occurs where more than one feature contains data that are similar.\n","\n","The effect of collinearity is that your model will overfit - you may get great results on your test data set, but then the model performs worse on unseen data (like the holdout set).\n","\n","One easy way to understand collinearity is with a simple binary variable like the **Sex** column in our dataset. Every passenger in our data is categorized as either male or female, so 'not male' is exactly the same as 'female'.\n","\n","As a result, when we created our two dummy columns from the categorical **Sex** column, we've actually created two columns with identical data in them. This will happen whenever we create dummy columns, and is called the [dummy variable trap](http://www.algosome.com/articles/dummy-variable-trap-regression.html). The easy solution is to choose one column to drop any time you make dummy columns.\n","\n","Collinearity can happen in other places, too. A common way to spot collinearity is to plot correlations between each pair of variables in a heatmap. An example of this style of plot is below:\n","\n","<img width=\"400\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1N9dZIsRqpUjfgSyR0d8casbdtS90CnXU\">\n","\n","\n","The darker squares, whether the darker red or darker blue, indicate pairs of columns that have higher correlation and may lead to collinearity. The easiest way to produce this plot is using the [DataFrame.corr()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) method to produce a correlation matrix, and then use the Seaborn library's [seaborn.heatmap()](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function to plot the values:\n","\n",">```python\n","import seaborn as sns\n","correlations = train.corr()\n","sns.heatmap(correlations)\n","plt.show()\n","```\n","\n","The example plot above was produced using a [code example from seaborn's documentation](http://seaborn.pydata.org/examples/many_pairwise_correlations.html) which produces an correlation heatmap that is easier to interpret than the default output of **heatmap()**. We've created a function containing that code to make it easier for you to plot the correlations between the features in our data."]},{"metadata":{"id":"mloADs3XlJqv","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import seaborn as sns\n","\n","def plot_correlation_heatmap(df):\n","    corr = df.corr()\n","    \n","    sns.set(style=\"white\")\n","    mask = np.zeros_like(corr, dtype=np.bool)\n","    mask[np.triu_indices_from(mask)] = True\n","\n","    f, ax = plt.subplots(figsize=(11, 9))\n","    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","\n","\n","    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n","            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n","    plt.show()\n","\n","columns = ['Age_categories_Missing', 'Age_categories_Infant',\n","       'Age_categories_Child', 'Age_categories_Teenager',\n","       'Age_categories_Young Adult', 'Age_categories_Adult',\n","       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n","       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n","       'SibSp_scaled', 'Parch_scaled', 'Fare_categories_0-12',\n","       'Fare_categories_12-50','Fare_categories_50-100', 'Fare_categories_100+',\n","       'Title_Master', 'Title_Miss', 'Title_Mr','Title_Mrs', 'Title_Officer',\n","       'Title_Royalty', 'Cabin_type_A','Cabin_type_B', 'Cabin_type_C', 'Cabin_type_D',\n","       'Cabin_type_E','Cabin_type_F', 'Cabin_type_G', 'Cabin_type_T', 'Cabin_type_Unknown']\n","plot_correlation_heatmap(train[columns])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"isyNfFNYlJqz","colab_type":"text"},"cell_type":"markdown","source":["# 9 - Final Feature Selection using RFECV\n"]},{"metadata":{"id":"gpC0FJVB0DWa","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","From the plot above we can see that there is a high correlation between **Sex_female/Sex_male** and **Title_Miss/Title_Mr/Title_Mrs**. We will remove the columns **Sex_female** and **Sex_male** since the title data may be more nuanced.\n","\n","Apart from that, we should remove one of each of our dummy variables to reduce the collinearity in each. We'll remove:\n","\n","- Pclass_2\n","- Age_categories_Teenager\n","- Fare_categories_12-50\n","- Title_Master\n","- Cabin_type_A\n","\n","In an earlier step, we manually used the logit coefficients to select the most relevant features. An alternate method is to use one of scikit-learn's inbuilt feature selection classes. We will be using the [feature_selection.RFECV](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) class which performs **recursive feature elimination** with cross-validation.\n","\n","The **RFECV** class starts by training a model using all of your features and scores it using cross validation. It then uses the logit coefficients to eliminate the least important feature, and trains and scores a new model. At the end, the class looks at all the scores, and selects the set of features which scored highest.\n","\n","Like the **LogisticRegression** class, **RFECV** must first be instantiated and then fit. The first parameter when creating the **RFECV** object must be an estimator, and we need to use the **cv** parameter to specific the number of folds for cross-validation.\n","\n",">```python\n","from sklearn.feature_selection import RFECV\n","lr = LogisticRegression()\n","selector = RFECV(lr,cv=10)\n","selector.fit(all_X,all_y)\n","```\n","\n","Once the **RFECV** object has been fit, we can use the **RFECV.support_** attribute to access a boolean mask of True and False values which we can use to generate a list of optimized columns:\n","\n",">```python\n","optimized_columns = all_X.columns[selector.support_]\n","```"]},{"metadata":{"id":"ORbJWVLslJqz","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.feature_selection import RFECV\n","\n","columns = ['Age_categories_Missing', 'Age_categories_Infant',\n","       'Age_categories_Child', 'Age_categories_Young Adult',\n","       'Age_categories_Adult', 'Age_categories_Senior', 'Pclass_1', 'Pclass_3',\n","       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'SibSp_scaled',\n","       'Parch_scaled', 'Fare_categories_0-12', 'Fare_categories_50-100',\n","       'Fare_categories_100+', 'Title_Miss', 'Title_Mr', 'Title_Mrs',\n","       'Title_Officer', 'Title_Royalty', 'Cabin_type_B', 'Cabin_type_C',\n","       'Cabin_type_D', 'Cabin_type_E', 'Cabin_type_F', 'Cabin_type_G',\n","       'Cabin_type_T', 'Cabin_type_Unknown']\n","\n","all_X = train[columns]\n","all_y = train[\"Survived\"]\n","lr = LogisticRegression()\n","selector = RFECV(lr,cv=10)\n","selector.fit(all_X,all_y)\n","\n","optimized_columns = all_X.columns[selector.support_]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fFO1SKYwlJq3","colab_type":"code","colab":{}},"cell_type":"code","source":["optimized_columns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ukq62yu_lJq6","colab_type":"text"},"cell_type":"markdown","source":["# 10 - Training A Model Using our Optimized Columns\n"]},{"metadata":{"id":"23y7lxrO1s8_","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The **RFECV()** selector returned only four columns:\n","\n",">```python\n","['SibSp_scaled', 'Title_Mr', 'Title_Officer', 'Cabin_type_Unknown']\n","```\n","\n","Let's train a model using cross validation using these columns and check the score."]},{"metadata":{"id":"5PuIl0M0lJq7","colab_type":"code","colab":{}},"cell_type":"code","source":["all_X = train[optimized_columns]\n","all_y = train[\"Survived\"]\n","\n","lr = LogisticRegression()\n","\n","scores = cross_val_score(lr, all_X, all_y, cv=10)\n","accuracy = scores.mean()\n","accuracy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0KZEPQaBlJrA","colab_type":"text"},"cell_type":"markdown","source":["# 11 - Submitting our Model to Kaggle\n"]},{"metadata":{"id":"mGjRqtHk1xUn","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","This four-feature model scores 82.26%, a modest improvement compared to the 81.5% from our earlier model. Let's train these columns on the holdout set, save a submission file and see what score we get from Kaggle.\n"]},{"metadata":{"id":"-nqKD5dllJrB","colab_type":"code","colab":{}},"cell_type":"code","source":["lr = LogisticRegression()\n","lr.fit(all_X,all_y)\n","holdout_predictions = lr.predict(holdout[optimized_columns])\n","\n","holdout_ids = holdout[\"PassengerId\"]\n","submission_df = {\"PassengerId\": holdout_ids,\n","                 \"Survived\": holdout_predictions}\n","submission = pd.DataFrame(submission_df)\n","\n","submission.to_csv(\"submission_3.csv\",index=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SRH8M_EAlJrD","colab_type":"text"},"cell_type":"markdown","source":["<img width=\"500\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1g_9YwzJB1I2c9hEWVsNN6LdfCo2dK2qN\">"]},{"metadata":{"id":"IW-LtvWelJrE","colab_type":"text"},"cell_type":"markdown","source":["# 12 - Next Steps\n"]},{"metadata":{"id":"8I0q4tGU12VO","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","The score this submission gets is 78.46%, which is equivalent to a jump of roughly 1,983 spots (again, this will vary as submission are constantly being made to the leaderboard).\n","\n","By preparing, engineering and selecting features, we have increased our accuracy by 2.4%. When working in Kaggle competitions, you should spend a lot of time experimenting with features, particularly feature engineering.\n","\n","Here are some ideas that you can use to work with features for this competition:\n","\n","- Use **SibSp** and **Parch** to explore total relatives onboard.\n","- Create combinations of multiple columns, for instance **Pclass + Sex**.\n","- See if you can extract useful data out of the **Ticket** column.\n","- Try different combinations of features to see if you can identify features that overfit less than others.\n","\n","In the next mission in this course, we'll look at selecting and optimizing different models to improve our score."]}]}